import time

# mpl.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

from kgcnn.data.mutagen.mutag import mutag_graph
from kgcnn.literature.INorp import make_inorp
from kgcnn.utils.data import ragged_tensor_from_nested_numpy
from kgcnn.utils.learning import lr_lin_reduction

# Download and compute dataset
labels, nodes, edge_indices, edges = mutag_graph()
labels[labels < 0] = 0
labels = np.expand_dims(labels, axis=-1)
graph_state = np.array([len(x) for x in nodes])

# Make train test split
labels_train, labels_test, nodes_train, nodes_test, edges_train, edges_test, edge_indices_train, edge_indices_test, graph_state_train, graph_state_test = train_test_split(
    labels, nodes, edges, edge_indices, graph_state, test_size=0.10, random_state=42)

# Convert to tf.RaggedTensor or tf.tensor
# adj_matrix copy of the data is generated by ragged_tensor_from_nested_numpy()
nodes_train, edges_train, edge_indices_train, graph_state_train = ragged_tensor_from_nested_numpy(
    nodes_train), ragged_tensor_from_nested_numpy(edges_train), ragged_tensor_from_nested_numpy(
    edge_indices_train), tf.constant(graph_state_train)

nodes_test, edges_test, edge_indices_test, graph_state_test = ragged_tensor_from_nested_numpy(
    nodes_test), ragged_tensor_from_nested_numpy(edges_test), ragged_tensor_from_nested_numpy(
    edge_indices_test), tf.constant(graph_state_test)

xtrain = nodes_train, edges_train, edge_indices_train, graph_state_train
xtest = nodes_test, edges_test, edge_indices_test, graph_state_test
ytrain = labels_train
ytest = labels_test

model = make_inorp(
    input_node_shape=[None],
    input_edge_shape=[None],
    input_state_shape=[],
    input_embedd= {"input_node_vocab" : 60, "input_edge_vocab" : 4, "input_state_vocab": 30, "input_node_embedd" : 16,
                    "input_edge_embedd" : 8, "input_state_embedd": 16, "input_type" : 'ragged'},
    # Output
    output_embedd= {"output_mode": 'graph', "output_type": 'padded'},
    output_mlp= {"units": [16, 8, 1], "use_bias" : [True, True, False], "activation" : ['relu', 'relu', 'sigmoid'] },
    # Model
    depth=1,
    node_mlp_args = {"units" : [16,16], "use_bias" : True, "activation" : ['relu',"linear"]},
    edge_mlp_args = {"units" : [16, 16], "use_bias" : True, "activation" : ['relu', "linear"]},
    pooling_args= {'is_sorted': False, 'has_unconnected': True, 'pooling_method': "segment_mean"},
    use_set2set =False,  # not in original paper
)

# Define learning rate and epochs
learning_rate_start = 1e-4
learning_rate_stop = 1e-5
epo = 500
epomin = 400
epostep = 10

# Compile model and optimizer
optimizer = tf.keras.optimizers.Adam(lr=learning_rate_start)
cbks = tf.keras.callbacks.LearningRateScheduler(lr_lin_reduction(learning_rate_start, learning_rate_stop, epomin, epo))
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
print(model.summary())

# Start and time training
start = time.process_time()
hist = model.fit(xtrain, ytrain,
                 epochs=epo,
                 batch_size=32,
                 callbacks=[cbks],
                 validation_freq=epostep,
                 validation_data=(xtest, ytest),
                 verbose=2
                 )
stop = time.process_time()
print("Print Time for taining: ", stop - start)

trainlossall = np.array(hist.history['accuracy'])
testlossall = np.array(hist.history['val_accuracy'])

acc_valid = testlossall[-1]

# Plot loss vs epochs
plt.figure()
plt.plot(np.arange(trainlossall.shape[0]), trainlossall, label='Training ACC', c='blue')
plt.plot(np.arange(epostep, epo + epostep, epostep), testlossall, label='Test ACC', c='red')
plt.scatter([trainlossall.shape[0]], [acc_valid], label="{0:0.4f} ".format(acc_valid), c='red')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Interaction Network Loss')
plt.legend(loc='upper right', fontsize='x-large')
plt.savefig('inorp_loss.png')
plt.show()
