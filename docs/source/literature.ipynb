{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc2d682",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac186e",
   "metadata": {},
   "source": [
    "There are a set of popular graph network architectures implemented already in `kgcnn` . They can be found in `kgcnn.literature` . Most models are set up in the functional ``keras`` API. Information on hyperparameters, training and benchmarking can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c829a4",
   "metadata": {},
   "source": [
    "* **[GCN](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GCN)**: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) by Kipf et al. (2016)\n",
    "* **[Schnet](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/Schnet)**: [SchNet – A deep learning architecture for molecules and materials ](https://aip.scitation.org/doi/10.1063/1.5019779) by Schütt et al. (2017)\n",
    "* **[GAT](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GAT)**: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) by Veličković et al. (2018)\n",
    "* **[GraphSAGE](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GraphSAGE)**: [Inductive Representation Learning on Large Graphs](http://arxiv.org/abs/1706.02216) by Hamilton et al. (2017)\n",
    "* **[GNNExplainer](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GNNExplain)**: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) by Ying et al. (2019)\n",
    "* **[AttentiveFP](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/AttentiveFP)**: [Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) by Xiong et al. (2019)\n",
    "* **[GATv2](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GATv2)**: [How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491) by Brody et al. (2021)\n",
    "* **[GIN](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/GIN)**: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) by Xu et al. (2019)\n",
    "* **[PAiNN](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/PAiNN)**: [Equivariant message passing for the prediction of tensorial properties and molecular spectra](https://arxiv.org/pdf/2102.03150.pdf) by Schütt et al. (2020)\n",
    "* **[DMPNN](https://github.com/aimat-lab/gcnn_keras/blob/master/kgcnn/literature/DMPNN)**: [Analyzing Learned Molecular Representations for Property Prediction](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00237) by Yang et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4a09",
   "metadata": {},
   "source": [
    "## Training Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50a513",
   "metadata": {},
   "source": [
    "Currently there are training scripts *train_graph.py*, *train_node.py*, *train_force.py*.\n",
    "\n",
    "> **NOTE**: They are quite integrated with ``kgcnn`` models and datasets which is why a custom training script can be favorable for models not in `kgcnn.literature`.\n",
    "\n",
    "Training scripts can be started with:\n",
    "\n",
    "```bash\n",
    "python3 train_node.py --hyper hyper/hyper_cora.py --category GCN\n",
    "python3 train_graph.py --hyper hyper/hyper_esol.py --category GIN\n",
    "```\n",
    "\n",
    "Where `hyper_esol.py` stores hyperparameter and must be in the same folder or a path to a `.py`. \n",
    "\n",
    "In principle, training can be fully configured with a serialized hyper parameter file as 'hyper.json' or 'hyper.yaml'.\n",
    "If a pyhton file 'hyper.py' is used then `hyper = {...}` must be set in the python script, in which case the items do not necessarily need to be in serailized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f44f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    \"info\":{ \n",
    "        # General information for training run\n",
    "        \"kgcnn_version\": \"4.0.0\", # Version \n",
    "        \"postfix\": \"\" # Postfix for output folder.\n",
    "    },\n",
    "    \"model\": { \n",
    "        # Model specific parameter, see kgcnn.literature.\n",
    "    },\n",
    "    \"data\": { \n",
    "        # Data specific parameters.\n",
    "    },\n",
    "    \"dataset\": { \n",
    "        # Dataset specific parameters.\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"fit\": { \n",
    "            # serialized keras fit arguments.\n",
    "        },\n",
    "        \"compile\": { \n",
    "            # serialized keras compile arguments.\n",
    "        },\n",
    "        \"cross_validation\": {\n",
    "            # serialized parameters for cross-validation.  \n",
    "        },\n",
    "        \"scaler\": {\n",
    "            # serialized parameters for scaler.\n",
    "            # Only add when training for regression.\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1e859",
   "metadata": {},
   "source": [
    "#### Data hyperparameter\n",
    "\n",
    "The kwargs for the dataset are not fully identical and vary a little depending on the datset. However, the most common are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper.update({\n",
    "    \"data\":{\n",
    "        # Other optinal entries (depends on the training script)\n",
    "        \"data_unit\": \"mol/L\",\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"class_name\": \"QM9Dataset\", # Name of the dataset\n",
    "        \"module_name\": \"kgcnn.data.datasets.QM9Dataset\",\n",
    "        \n",
    "        # Config like filepath etc., leave empty for pre-defined datasets\n",
    "        \"config\": {}, \n",
    "        \n",
    "        # Methods to run on dataset, i.e. the list of graphs\n",
    "        \"methods\": [\n",
    "            {\"prepare_data\": {}}, # Used for cache and pre-compute data, leave out for pre-defined datasets\n",
    "            {\"read_in_memory\": {}}, # Used for reading into memory, leave out for pre-defined datasets\n",
    "            \n",
    "            # Example method to run over each graph in the list using `map_list` method.\n",
    "            # The string 'set_range' refers to a preprocessor. Legacy short access to graph preprocessors.\n",
    "            {\"map_list\": {\"method\": \"set_range\", \"max_distance\": 4, \"max_neighbours\": 30}},\n",
    "            {\"map_list\": {\"method\": \"count_nodes_and_edges\", \"total_edges\": \"total_edges\",\n",
    "                          \"count_edges\": \"edge_indices\", \"count_nodes\": \"node_attributes\", \"total_nodes\": \"total_nodes\"}},\n",
    "        ]\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c48451",
   "metadata": {},
   "source": [
    "#### Model hyperparameter\n",
    "\n",
    "The model parameters can be reviewed from the default values in ``kgcnn.literature``. Mostly model input and output has to be matched depending on the data representation. That is type of input and its shape. An input-type checker can be used from `kgcnn.data.base.MemoryGraphDataset`, which has `assert_valid_model_input`. In ``inputs`` a list of kwargs must be given, which are each unpacked in the corresponding ``tf.keras.layers.Input``. The order matters and is model dependent.\n",
    "\n",
    "Moreover, naming of the model input is used to link the tensor properties of the dataset with the model input. The output dimension of either node or graph embedding can be set for most models with the \"output_mlp\" argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b69e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper.update({\n",
    "    \"model\":{\n",
    "        \"module_name\": \"kgcnn.literature.GCN\", \n",
    "        \"class_name\": \"make_model\",\n",
    "        \"config\":{\n",
    "            \"inputs\": [\n",
    "                {\"shape\": [None, 100], \"name\": \"node_attributes\", \"dtype\": \"float32\"},\n",
    "                {\"shape\": [None, 2], \"name\": \"edge_indices\", \"dtype\": \"int64\"},\n",
    "                {\"shape\": (), \"name\": \"total_nodes\", \"dtype\": \"int64\"},\n",
    "                {\"shape\": (), \"name\": \"total_edges\", \"dtype\": \"int64\"}\n",
    "            ],\n",
    "            # More model specific kwargs, like:\n",
    "            \"depth\": 5,\n",
    "            # Output part defining model output\n",
    "            \"output_embedding\": \"graph\",\n",
    "            \"output_mlp\": {\"use_bias\": [True, True, False], \"units\": [140, 70, 70],\n",
    "                           \"activation\": [\"relu\", \"relu\", \"softmax\"]}\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f3bf1",
   "metadata": {},
   "source": [
    "#### Training hyperparameter\n",
    "\n",
    "The kwargs for training simply sets arguments for ``model.compile(**kwargs)`` and ``model.fit(**kwargs)`` that matches keras arguments as well as for the k-fold split from scikit-learn. The kwargs are expected to be fully serialized, if the hyper parameters are supposed to be saved to json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    }
   ],
   "source": [
    "import keras_core as ks\n",
    "hyper.update({\n",
    "    \"training\":{\n",
    "        # Cross-validation of the data\n",
    "        \"cross_validation\": {\n",
    "            \"class_name\": \"KFold\",\n",
    "            \"config\": {\"n_splits\": 5, \"random_state\": 42, \"shuffle\": True}\n",
    "        },\n",
    "        # Standard scaler for regression targets\n",
    "        \"scaler\": {\n",
    "            \"class_name\": \"StandardScaler\",\n",
    "            \"module_name\": \"kgcnn.data.transform.scaler.standard\",\n",
    "            \"config\": {\"with_std\": True, \"with_mean\": True, \"copy\": True}\n",
    "        },\n",
    "        # Keras model compile and fit\n",
    "        \"compile\": {\n",
    "            \"loss\": \"categorical_crossentropy\",\n",
    "            \"optimizer\": ks.saving.serialize_keras_object(\n",
    "                ks.optimizers.Adam(learning_rate=0.001))\n",
    "        },\n",
    "        \"fit\": {\n",
    "            \"batch_size\": 32, \"epochs\": 800, \"verbose\": 2, \n",
    "            \"callbacks\": []\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f65e0c",
   "metadata": {},
   "source": [
    "#### Info\n",
    "\n",
    "Some general information on the training, such as the used kgcnn version or a postfix for the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper.update({\n",
    "    \"info\":{ # Generla information\n",
    "        \"postfix\": \"_v1\", # Appends _v1 to output folder\n",
    "        \"postfix_file\": \"_run2\", # Appends _run2 to info files\n",
    "        \"kgcnn_version\": \"4.0.0\"    \n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd8726",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8356b6",
   "metadata": {},
   "source": [
    "# Summary of Benchmark Training\n",
    "\n",
    "Note that these are the results for models within `kgcnn` implementation, and that training is not always done with optimal hyperparameter or splits, when comparing with literature.\n",
    "This table is generated automatically from keras history logs.\n",
    "Model weights and training statistics plots are not uploaded on \n",
    "[github](https://github.com/aimat-lab/gcnn_keras/tree/master/training/results) \n",
    "due to their file size.\n",
    "\n",
    "*Max.* or *Min.* denotes the best test error observed for any epoch during training.\n",
    "To show overall best test error run ``python3 summary.py --min_max True``.\n",
    "If not noted otherwise, we use a (fixed) random k-fold split for validation errors.\n",
    "\n",
    "#### CoraLuDataset\n",
    "\n",
    "Cora Dataset after Lu et al. (2003) of 2708 publications and 1433 sparse attributes and 7 node classes. Here we use random 5-fold cross-validation on nodes. \n",
    "\n",
    "| model     | kgcnn   |   epochs | Categorical accuracy   |\n",
    "|:----------|:--------|---------:|:-----------------------|\n",
    "| GAT       | 4.0.0   |      250 | 0.8464 &pm; 0.0105     |\n",
    "| GATv2     | 4.0.0   |      250 | 0.8331 &pm; 0.0104     |\n",
    "| GCN       | 4.0.0   |      300 | 0.8072 &pm; 0.0109     |\n",
    "| GIN       | 4.0.0   |      500 | 0.8279 &pm; 0.0170     |\n",
    "| GraphSAGE | 4.0.0   |      500 | **0.8497 &pm; 0.0100** |\n",
    "\n",
    "#### ESOLDataset\n",
    "\n",
    "ESOL consists of 1128 compounds as smiles and their corresponding water solubility in log10(mol/L). We use random 5-fold cross-validation. \n",
    "\n",
    "| model   | kgcnn   |   epochs | MAE [log mol/L]        | RMSE [log mol/L]       |\n",
    "|:--------|:--------|---------:|:-----------------------|:-----------------------|\n",
    "| GAT     | 4.0.0   |      500 | 0.4826 &pm; 0.0255     | 0.6903 &pm; 0.0705     |\n",
    "| GCN     | 4.0.0   |      800 | **0.4623 &pm; 0.0224** | **0.6567 &pm; 0.0456** |\n",
    "| Schnet  | 4.0.0   |      800 | 0.4678 &pm; 0.0227     | 0.6662 &pm; 0.0629     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9986d0d",
   "metadata": {},
   "source": [
    "> **NOTE**: You can find this page as jupyter notebook in https://github.com/aimat-lab/gcnn_keras/tree/master/docs/source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
